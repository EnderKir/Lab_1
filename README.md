# Lab_1

В первой лабораторной работе мы реализовали полносвязную нейронную сеть из 5 слоёв.
Сначала добавляем Flatten-слой, разворачивающий нашу матрицу изображений в одномерную для дальнейшего поступления на Dense-слой.
Dense: 2 слоя на 128 нейронов, еще 2 слоя на 64 нейрона и слой на 10 нейронов.

Функции активации relu, на выходе softmax.

В цикле обучения используются:
* оптимизатор 'adam'
* функция потерь 'sparse_categorical_crossentropy'
* метрика 'accuracy'

Как ReLU сравнивает:
ReLU является линейным (тождественным) для всех положительных значений и нулем для всех отрицательных значений.

Функция Softmax, замечательная функция активации, которая превращает числа или логиты в вероятности, которые равны единице. Функция Softmax выводит вектор, который представляет распределения вероятностей списка потенциальных результатов

При обучении нейронной сети происходит за 50 эпох


Разреженная категориальная перекрестная энтропия /
sparse categorical crossentropy: 
![sparse](https://i.ibb.co/fYV0f5L/photo-2020-04-07-17-02-26.jpg)

loss: 1.4366 - acc: 0.4827 - val_loss: 1.5164 - val_acc: 0.4581

## Графики метрики точности и функции потерь:

![acc](https://i.ibb.co/Bjys3VJ/acc.jpg)

## Графики метрики точности и функции потерь на валидационной выборке:

![val_acc](https://i.ibb.co/hcwYYkS/val-acc.jpg)
